{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69029cae",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Write a python program to scrape data for “Data Analyst” Job position in\n",
    "“Bangalore” location. You have to scrape the job-title, job-location, company_name,\n",
    "experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore”\n",
    "in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd9a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66cd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job-title, job-location, company_name,\n",
    "# experience_required.\n",
    "\n",
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = uc.Chrome(executable_path='/home/keshav/chromedriver', options=options)\n",
    "\n",
    "# enter keyword to search (URL)\n",
    "keyword = \"https://www.naukri.com/\"\n",
    "driver.get(keyword)\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc51017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the 3 popup windows\n",
    "for _ in range(3):\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    time.sleep(1)\n",
    "    driver.close()\n",
    "driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ab8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job skill\n",
    "Job_skill_position = \"Data Analyst\"\n",
    "skill_box = driver.find_element(By.XPATH,\"//input[@placeholder='Skills, Designations, Companies']\")\n",
    "skill_box.send_keys(Job_skill_position)\n",
    "\n",
    "# location\n",
    "location = \"Bangalore\"\n",
    "location_box = driver.find_element(By.XPATH,\"//input[@placeholder='Enter Locations…']\")\n",
    "location_box.send_keys(location)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# search button\n",
    "driver.find_element(By.XPATH, \"//button[text()='Search']\").click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job postings\n",
    "articles = driver.find_elements(By.XPATH,\"//article[@class='jobTuple bgWhite br4 mb-8']\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a285bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "columns = [\"Job Title\",' Company', 'Experience Req.','Location']\n",
    "\n",
    "# parse data\n",
    "for article in articles:\n",
    "    \n",
    "    # job title\n",
    "    title = article.find_element(By.XPATH,\".//a[@class='title fw500 ellipsis']\").text.strip()\n",
    "    \n",
    "    # company names\n",
    "    company = article.find_element(By.XPATH,\".//a[@class='subTitle ellipsis fleft']\").text.strip()\n",
    "    \n",
    "    # experience \n",
    "    experience = article.find_element(By.XPATH,\".//i[@class='fleft icon-16 lh16 mr-4 naukicon naukicon-experience']/ following-sibling::*\") \\\n",
    "                        .text.strip()\n",
    "    # location\n",
    "    location = article.find_element(By.XPATH,\".//li[@class='fleft grey-text br2 placeHolderLi location']\") \\\n",
    "                      .text.strip()\n",
    "    \n",
    "    data.append([title, company, experience, location])\n",
    "#     print(\"Title: \",title, \"Company: \", company)\n",
    "#     print(\"Experience: \", experience, \"Location: \", location)\n",
    "#     print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.index = df.index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556753d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(Job_skill_position+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4506a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ced961",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Write a python program to scrape data for “Data Scientist” Job position in\n",
    "“Bangalore” location. You have to scrape the job-title, job-location,\n",
    "company_name, full job-description. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter\n",
    "“Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1079c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job-title, job-location, company_name,\n",
    "# experience_required.\n",
    "\n",
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = uc.Chrome(executable_path='/home/keshav/chromedriver', options=options)\n",
    "\n",
    "# enter keyword to search (URL)\n",
    "keyword = \"https://www.naukri.com/\"\n",
    "driver.get(keyword)\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the 3 popup windows\n",
    "for _ in range(3):\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    time.sleep(1)\n",
    "    driver.close()\n",
    "driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job skill\n",
    "Job_skill_position = \"Data Scientist\"\n",
    "skill_box = driver.find_element(By.XPATH,\"//input[@placeholder='Skills, Designations, Companies']\")\n",
    "skill_box.send_keys(Job_skill_position)\n",
    "\n",
    "# location\n",
    "location = \"Bangalore\"\n",
    "location_box = driver.find_element(By.XPATH,\"//input[@placeholder='Enter Locations…']\")\n",
    "location_box.send_keys(location)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# search button\n",
    "driver.find_element(By.XPATH, \"//button[text()='Search']\").click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ddbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists for scraping data\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "full_job_description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job postings\n",
    "articles = driver.find_elements(By.XPATH,\"//article[@class='jobTuple bgWhite br4 mb-8']\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "urls = []\n",
    "job_descriptions = []\n",
    "companies = []\n",
    "locations = []\n",
    "columns = [\"Job Title\",' Company','Location', \"Job Description\"]\n",
    "\n",
    "# parse data\n",
    "for article in articles:\n",
    "    \n",
    "    # job title\n",
    "    title = article.find_element(By.XPATH,\".//a[@class='title fw500 ellipsis']\").text.strip()\n",
    "    \n",
    "    # company names\n",
    "    company = article.find_element(By.XPATH,\".//a[@class='subTitle ellipsis fleft']\").text.strip()\n",
    "    \n",
    "    # for description\n",
    "    url = article.find_element(By.XPATH,\".//a[@class='title fw500 ellipsis']\").get_attribute('href')\n",
    "    urls.append(url)\n",
    "    \n",
    "    # location\n",
    "    location = article.find_element(By.XPATH,\".//li[@class='fleft grey-text br2 placeHolderLi location']\") \\\n",
    "                      .text.strip()\n",
    "    \n",
    "    titles.append(title)\n",
    "    companies.append(company)\n",
    "    locations.append(location)\n",
    "#     print(\"Title: \",title, \"Company: \", company)\n",
    "#     print(\"Location: \", location)\n",
    "#     print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get job descriptions\n",
    "for url in urls:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        raw_description=driver.find_element(By.XPATH,\"//section[@class='job-desc']/div[1]\").text\n",
    "        description=raw_description.replace(\"Contact Person\",\"@@@@@\")\n",
    "        description= description.split(\"@@@@@\")\n",
    "        job_description.append(description[0])\n",
    "    except NoSuchElementException :\n",
    "        job_description.append(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    columns[0] : titles,\n",
    "    columns[1] : companies,\n",
    "    columns[2] : locations,\n",
    "    columns[3] : job_descriptions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b6473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataframe from the scraped data and taking only first 10 jobs\n",
    "df=pd.DataFrame(data_dict)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2cc5c",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "In this question you have to scrape data using the filters available on the\n",
    "webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name,\n",
    "experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f766df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4371123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job-title, job-location, company_name,\n",
    "# experience_required.\n",
    "\n",
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = uc.Chrome(executable_path='/home/keshav/chromedriver', options=options)\n",
    "\n",
    "# enter keyword to search (URL)\n",
    "keyword = \"https://www.naukri.com/\"\n",
    "driver.get(keyword)\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a54fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the 3 popup windows\n",
    "for _ in range(3):\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    time.sleep(1)\n",
    "    driver.close()\n",
    "driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03edb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job skill\n",
    "Job_skill_position = \"Data Scientist\"\n",
    "skill_box = driver.find_element(By.XPATH,\"//input[@placeholder='Skills, Designations, Companies']\")\n",
    "skill_box.send_keys(Job_skill_position)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# search button\n",
    "driver.find_element(By.XPATH, \"//button[text()='Search']\").click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters\n",
    "# location filter\n",
    "driver.find_element(By.XPATH,\"//p[@class='grey-text lH20 fleft ml-8 txtLbl']//span[text()='Delhi / NCR']/..\") \\\n",
    "      .click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "# salary filter\n",
    "driver.find_element(By.XPATH, \"//p[@class='grey-text lH20 fleft ml-8 txtLbl']//span[text()='3-6 Lakhs']/..\") \\\n",
    "      .click()\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d2a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job postings\n",
    "articles = driver.find_elements(By.XPATH,\"//article[@class='jobTuple bgWhite br4 mb-8']\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e71afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "columns = [\"Job Title\",' Company', 'Experience Req.','Location']\n",
    "\n",
    "# parse data\n",
    "for article in articles:\n",
    "    \n",
    "    # job title\n",
    "    title = article.find_element(By.XPATH,\".//a[@class='title fw500 ellipsis']\").text.strip()\n",
    "    \n",
    "    # company names\n",
    "    company = article.find_element(By.XPATH,\".//a[@class='subTitle ellipsis fleft']\").text.strip()\n",
    "    \n",
    "    # experience \n",
    "    experience = article.find_element(By.XPATH,\".//i[@class='fleft icon-16 lh16 mr-4 naukicon naukicon-experience']/ following-sibling::*\") \\\n",
    "                        .text.strip()\n",
    "    # location\n",
    "    location = article.find_element(By.XPATH,\".//li[@class='fleft grey-text br2 placeHolderLi location']\") \\\n",
    "                      .text.strip()\n",
    "    \n",
    "    data.append([title, company, experience, location])\n",
    "#     print(\"Title: \",title, \"Company: \", company)\n",
    "#     print(\"Experience: \", experience, \"Location: \", location)\n",
    "#     print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8095e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf9cea",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "Write a python program to scrape data for first 10 job results for Data scientist\n",
    "Designation in Noida location. You have to scrape company_name, No. of days\n",
    "ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida”\n",
    "in “location” field.\n",
    "3. Then click the search button. You will land up in the below page:\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown\n",
    "page.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8100e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf02280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = uc.Chrome(executable_path='/home/keshav/chromedriver', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93639080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter keyword to search (URL)\n",
    "keyword = \"https://www.glassdoor.co.in/index.htm\"\n",
    "driver.get(keyword)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22209959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign in button in navbar\n",
    "driver.find_element(By.XPATH, \"//button[text()='Sign In' and @class='d-none d-lg-block p-0 LockedHomeHeaderStyles__signInButton']\") \\\n",
    "      .click()\n",
    "\n",
    "# email box\n",
    "driver.find_element(By.XPATH, \"//*[@id='userEmail']\").send_keys(email)\n",
    "\n",
    "# password box\n",
    "driver.find_element(By.XPATH, \"//*[@id='userPassword']\").send_keys(pwd)\n",
    "\n",
    "# sign in\n",
    "driver.find_element(By.XPATH, \"//button[text()='Sign In' and @class='gd-ui-button minWidthBtn css-8i7bc2']\") \\\n",
    "      .click()\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ab5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword job skill\n",
    "driver.find_element(By.XPATH, \"//input[@id='sc.keyword']\").clear()\n",
    "time.sleep(0.5)\n",
    "driver.find_element(By.XPATH, \"//input[@id='sc.keyword']\").send_keys(\"Data Scientist\")\n",
    "\n",
    "# location\n",
    "time.sleep(0.5)\n",
    "driver.find_element(By.XPATH, \"//input[@class='css-1etjok6' and @placeholder='Location']\").clear()\n",
    "time.sleep(0.5)\n",
    "driver.find_element(By.XPATH, \"//*[@id='sc.location']\").send_keys(\"Noida\")\n",
    "\n",
    "# search button\n",
    "driver.find_element(By.XPATH, \"//*[@id='scBar']/div/button\").click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "postings = driver.find_elements(By.XPATH, \"//*[@id='MainCol']/div[1]/ul/li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "columns = [\"Company\",\"Job Post Age\",\"Rating\"]\n",
    "for posting in postings:\n",
    "    \n",
    "    company = posting.find_element(By.XPATH,\".//div[@class='d-flex justify-content-between align-items-start']\") \\\n",
    "                     .text\n",
    "    days = posting.find_element(By.XPATH,\".//div[@class='d-flex align-items-end pl-std css-17n8uzw']\") \\\n",
    "                  .text\n",
    "    try:\n",
    "        rating = posting.find_element(By.XPATH, \".//span[@class=' job-search-key-srfzj0 e1cjmv6j0']\") \\\n",
    "                        .text\n",
    "    except:\n",
    "        rating = \"-\"\n",
    "    \n",
    "    data.append([company,days,rating])\n",
    "#     print(\"Company: \", company.strip())\n",
    "#     print(\"Days: \", days.strip())\n",
    "#     print(\"Rating: \", rating.strip())\n",
    "#     print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data[:10], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4b9fc",
   "metadata": {},
   "source": [
    "### Question 5: (correct this question)\n",
    "Write a python program to scrape the salary data for Data Scientist designation\n",
    "in Noida location.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min\n",
    "salary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the below page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766fedf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = uc.Chrome(executable_path='/home/keshav/chromedriver', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba960ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter keyword to search (URL)\n",
    "keyword = \"https://www.glassdoor.co.in/index.htm\"\n",
    "driver.get(keyword)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"keshav_bansal_ML@gmail.com\"\n",
    "pwd = \"Aza3WhTn@MQk5Pn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign in button in navbar\n",
    "driver.find_element(By.XPATH, \"//button[text()='Sign In' and @class='d-none d-lg-block p-0 LockedHomeHeaderStyles__signInButton']\") \\\n",
    "      .click()\n",
    "\n",
    "# email box\n",
    "driver.find_element(By.XPATH, \"//*[@id='userEmail']\").send_keys(email)\n",
    "\n",
    "# password box\n",
    "driver.find_element(By.XPATH, \"//*[@id='userPassword']\").send_keys(pwd)\n",
    "\n",
    "# sign in\n",
    "driver.find_element(By.XPATH, \"//button[text()='Sign In' and @class='gd-ui-button minWidthBtn css-8i7bc2']\") \\\n",
    "      .click()\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e22d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.glassdoor.co.in/Salaries/index.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9714f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword job skill\n",
    "driver.find_element(By.XPATH, \"//*[@id='KeywordSearch']\").clear()\n",
    "time.sleep(0.5)\n",
    "driver.find_element(By.XPATH, \"//*[@id='KeywordSearch']\").send_keys(\"Data Scientist\")\n",
    "\n",
    "# location\n",
    "time.sleep(0.5)\n",
    "driver.find_element(By.XPATH, \"//*[@id='LocationSearch']\").clear()\n",
    "time.sleep(0.5)\n",
    "driver.find_element(By.XPATH, \"//*[@id='LocationSearch']\").send_keys(\"Noida\")\n",
    "\n",
    "# search button\n",
    "driver.find_element(By.XPATH, \"//*[@id='HeroSearchButton']\").click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78607e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Company','No of Salaries','Avg salary','Max Salary','Min Salary']\n",
    "data = []\n",
    "page_counter = 1\n",
    "while(page_counter < 4):\n",
    "    postings = driver.find_elements(By.XPATH, \"//div[@class='py css-17435dd']/div[@class='row']\")\n",
    "    \n",
    "    for posting in postings:    \n",
    "\n",
    "        company = posting.find_element(By.XPATH,\".//h3[@class='m-0 css-g261rn']/a\").text\n",
    "        no_of_salaries = posting.find_element(By.XPATH,\".//div[@class='col-12 col-lg-auto']\").text\n",
    "        avg_salary = posting.find_element(By.XPATH,\".//div[@class='col-12 col-lg-4 px-lg-0 d-flex align-items-baseline']\") \\\n",
    "                            .text\n",
    "\n",
    "        min_salary = posting.find_elements(By.XPATH,\".//p[@class='m-0 css-1vkj9it']\")[0].text\n",
    "        max_salary = posting.find_elements(By.XPATH,\".//p[@class='m-0 css-1vkj9it']\")[1].text\n",
    "        try:\n",
    "            rating = posting.find_element(By.XPATH, \".//span[@class='m-0 css-kyx745']\")\n",
    "\n",
    "        except:\n",
    "            rating = \"-\"\n",
    "\n",
    "        data.append([company.strip(),\n",
    "                    no_of_salaries.strip(),\n",
    "                    avg_salary.strip().replace(\"\\n\",\"\"),\n",
    "                    max_salary.strip(),\n",
    "                    min_salary.strip()])\n",
    "#         print(\"Company: \",company.strip())\n",
    "#         print(\"No of Salaries: \", no_of_salaries.strip())\n",
    "#         print(\"Avg salary: \", avg_salary.strip().replace(\"\\n\",\"\"))\n",
    "#         print(\"Max Salary: \", max_salary.strip()) \n",
    "#         print(\"Min Salary: \", min_salary.strip())\n",
    "#         print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        \n",
    "    try:\n",
    "        driver.find_element(By.XPATH,\"//button[@class='nextButton css-sed91k']\").click()\n",
    "        page_counter += 1\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced029f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19109bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c4b01",
   "metadata": {},
   "source": [
    "### Question 6: \n",
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to\n",
    "scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %\n",
    "\n",
    "##### The attributes which you have to scrape is ticked marked in the below image.\n",
    "#### Steps:\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and\n",
    "more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page\n",
    "you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of\n",
    "the page , then click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses.\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb381856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = uc.Chrome(executable_path='/home/keshav/chromedriver', options=options)\n",
    "\n",
    "\n",
    "# enter keyword to search (URL)\n",
    "keyword = \"https://www.flipkart.com/\"\n",
    "driver.get(keyword)\n",
    "\n",
    "time.sleep(4)\n",
    "# checking current window title\n",
    "print('Current page: ',driver.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6098da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close popup\n",
    "driver.find_element(By.XPATH,\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e03aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search box\n",
    "search_box = driver.find_element(By.XPATH,\"//input[@title='Search for products, brands and more']\")\n",
    "search_box.send_keys('sunglasses')\n",
    "time.sleep(1)\n",
    "\n",
    "# Search button\n",
    "search_button = driver.find_element(By.XPATH,\"//button[@class='L0Z3Pu']\").click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "data = []\n",
    "page_count = 1\n",
    "columns = [\"Company\",\"Description\",\"Price\",\"Discount\"]\n",
    "\n",
    "while (counter <=100):\n",
    "    \n",
    "    products = driver.find_elements(By.XPATH,\"//div[@class='_1xHGtK _373qXS']//div[@class='_2B099V']\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(f\"Products in page {page_count}: \",len(products))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    \n",
    "    for product in products:\n",
    "        \n",
    "        company = product.find_element(By.XPATH,\".//div[@class='_2WkVRV']\").text\n",
    "        data_row = product.text.split(\"\\n\")\n",
    "        try:\n",
    "            desc = product.find_element(By.XPATH,\".//a[@class='IRpwTa']\").text\n",
    "        except:\n",
    "            desc = product.find_element(By.XPATH,\".//a[@class='IRpwTa _2-ICcC']\").text\n",
    "        price = product.find_element(By.XPATH,\".//div[@class='_30jeq3']\").text\n",
    "        \n",
    "        try:\n",
    "            discount = product.find_element(By.XPATH,\".//div[@class='_3Ay6Sb']\").text\n",
    "        except:\n",
    "            discount = \"-\"\n",
    "        \n",
    "#         print(company) \n",
    "#         print(desc)\n",
    "#         print(price)\n",
    "#         print(discount)\n",
    "#         print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "#         print(counter)\n",
    "#         print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        if (counter>100):\n",
    "            break\n",
    "        else:\n",
    "            counter += 1        \n",
    "            data.append([company,desc,price,discount])\n",
    "        \n",
    "    page_count += 1\n",
    "    driver.find_elements(By.XPATH,\"//nav[@class='yFHi8N']/a\")[-1].click()\n",
    "    time.sleep(2)\n",
    "print(\"Done Scraping!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde8c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303c5ce",
   "metadata": {},
   "source": [
    "### Question 7:\n",
    "Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to\n",
    "go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\n",
    "When you will open the above link you will reach to the below shown webpage.\n",
    "As shown in the above page you have to scrape the tick marked attributes.\n",
    "These are\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "\n",
    "##### You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97abfa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = uc.Chrome(executable_path='/home/keshav/chromedriver', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter keyword to search (URL)\n",
    "keyword = \"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\"\n",
    "driver.get(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef703fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbcd0df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,\"//a/div[@class='_3UAT2v _16PBlm']/..\").click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e9734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "data = []\n",
    "\n",
    "while(counter<100):\n",
    "    reviews_deck = driver.find_elements(By.XPATH,\"//div[@class='col _2wzgFH K0kLPL']\")    \n",
    "    for review in reviews_deck:\n",
    "        \n",
    "        if(counter>100):\n",
    "            break\n",
    "        else:\n",
    "            counter+=1\n",
    "        try:\n",
    "            review.find_element(By.XPATH,\".//div[@class='row']//span[@class='_1BWGvX']\").click()\n",
    "        except:\n",
    "            pass\n",
    "        finally:\n",
    "            rating, review_desc = review.find_elements(By.XPATH,\".//div[@class='row']\")[:2]\n",
    "            rating_stars, rating_desc = rating.text[0],rating.text[1:]\n",
    "        \n",
    "        rating_stars_ = rating_stars.strip()\n",
    "        rating_desc_ = rating_desc.strip()\n",
    "        review_desc_ = review_desc.text.strip()\n",
    "        data.append([rating_stars_, rating_desc_, review_desc_])\n",
    "    try:\n",
    "        driver.find_element(By.XPATH,\"//a[@class='_1LKTO3'][2]/span\").click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        driver.find_element(By.XPATH,\"//a[@class='_1LKTO3']/span\").click()\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb074b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06805a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Rating', 'Rating Summary', 'Review']\n",
    "df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df05f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ace57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfefdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Flipkart Iphone reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b50cd4",
   "metadata": {},
   "source": [
    "### Question 8:\n",
    "Scrape data for first 100 sneakers you find when you visit flipkart.com and\n",
    "search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "##### Also note that all the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb32265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = uc.Chrome(executable_path='/home/keshav/chromedriver', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter keyword to search (URL)\n",
    "keyword = \"https://www.flipkart.com/\"\n",
    "driver.get(keyword)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # close popup\n",
    "    driver.find_element(By.XPATH,\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "    time.sleep(1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search box\n",
    "search_box = driver.find_element(By.XPATH,\"//input[@placeholder]\")\n",
    "search_box.send_keys('sneakers')\n",
    "time.sleep(1)\n",
    "\n",
    "# Search button\n",
    "search_button = driver.find_element(By.XPATH,\"//button[@class='L0Z3Pu']\").click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212db49f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "counter = 1\n",
    "while(counter<100):\n",
    "    \n",
    "    products = driver.find_elements(By.XPATH, \"//div[@class='_2B099V']\")\n",
    "\n",
    "    for product in products:    \n",
    "        if(counter>100):\n",
    "            break\n",
    "        else:\n",
    "            counter+=1\n",
    "        # company\n",
    "        company = product.find_element(By.XPATH,\".//div[@class='_2WkVRV']\").text.strip()\n",
    "        # description\n",
    "        try:\n",
    "            desc = product.find_element(By.XPATH,\".//a[@class='IRpwTa _2-ICcC']\").get_attribute('title')\n",
    "        except:\n",
    "            desc = product.find_element(By.XPATH,\".//a[@class='IRpwTa']\").get_attribute('title')\n",
    "        # price\n",
    "        price = product.find_element(By.XPATH,\".//div[@class='_30jeq3']\").text.strip()\n",
    "        # discount\n",
    "        discount = product.find_element(By.XPATH,\".//div[@class='_3Ay6Sb']\").text.strip()\n",
    "            \n",
    "        data.append([company,desc,price,discount])\n",
    "    driver.find_elements(By.XPATH,\"//a[@class='_1LKTO3']\")[-1].click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Company', 'Description', 'Price','Discount']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "df.index = df.index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fac24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347bd32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Flipkart Sneakers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab8d3b",
   "metadata": {},
   "source": [
    "### Question 9:\n",
    "Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image.\n",
    "Please note that applying the filter and scraping the data , everything should be done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = uc.Chrome(executable_path='/home/keshav/chromedriver', options=options)\n",
    "# enter keyword to search (URL)\n",
    "keyword = \"https://www.myntra.com/shoes\"\n",
    "driver.get(keyword)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color check\n",
    "driver.find_element(By.XPATH,\"//input[@value='Black']/..\").click()\n",
    "time.sleep(2)\n",
    "# price check\n",
    "driver.find_element(By.XPATH,\"//input[@value='6897.0 TO 13595.0']/..\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7b07c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 1\n",
    "data = []\n",
    "# for next page, doesn't work in first go, had to do it twice\n",
    "\n",
    "while(counter<100):\n",
    "    products = driver.find_elements(By.XPATH,\"//div[@class='product-productMetaInfo']\")\n",
    "    for product in products:\n",
    "        try:\n",
    "            brand = product.find_element(By.XPATH,\"./h3[@class='product-brand']\").text.strip()\n",
    "        except:\n",
    "            brand = '-'\n",
    "        try:\n",
    "            desc = product.find_element(By.XPATH,\"./h4[@class='product-product']\").text.strip()\n",
    "        except:\n",
    "            desc = '-'\n",
    "        try:\n",
    "            price = product.find_element(By.XPATH,\".//div[@class='product-price']\").text.strip()\n",
    "        except:\n",
    "            price = '-'\n",
    "        \n",
    "        if(counter>100):\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "   \n",
    "        data.append([brand,desc,price])\n",
    "        \n",
    "    print(\"In While Loop!!!\")    \n",
    "    time.sleep(6)\n",
    "    if (counter>=100):\n",
    "        break\n",
    "    driver.find_element(By.XPATH,\"//a[@rel='next']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Brand\",\"Description\",\"Price\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.index = df.index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6779a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('myntra shoes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92a4b4",
   "metadata": {},
   "source": [
    "### Question 10:\n",
    "Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the\n",
    "below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes\n",
    "for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bd4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for chromedriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = uc.Chrome(executable_path='/home/keshav/chromedriver', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter keyword to search (URL)\n",
    "keyword = \"https://www.amazon.in/\"\n",
    "driver.get(keyword)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search box\n",
    "search_box = driver.find_element(By.XPATH,\"//input[@type='text']\")\n",
    "search_box.send_keys('Laptop')\n",
    "time.sleep(1)\n",
    "\n",
    "# Search button\n",
    "search_button = driver.find_element(By.XPATH,\"//input[@id='nav-search-submit-button']\").click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # i7\n",
    "    driver.find_element(By.XPATH,\"//li[@aria-label='Intel Core i7']/span/a\").click()\n",
    "    time.sleep(3)\n",
    "except:\n",
    "    print(\"Error\")\n",
    "\n",
    "try:\n",
    "    # i9\n",
    "    driver.find_element(By.XPATH,\"//li[@aria-label='Intel Core i9']/span/a\")\n",
    "    time.sleep(2)\n",
    "except:\n",
    "    print(\"Error 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e471d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops = driver.find_elements(By.XPATH,\"//div[@data-component-type='s-search-result']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(laptops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac2f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "data = []\n",
    "for laptop in laptops:\n",
    "    try:\n",
    "        # title\n",
    "        title = laptop.find_element(By.XPATH,\".//span[@class='a-size-medium a-color-base a-text-normal']\").text.strip()\n",
    "        # rating\n",
    "        rating = laptop.find_element(By.XPATH,\".//span[@class='a-icon-alt']/..\")\n",
    "        rating = rating.get_attribute('innerHTML').split(\">\")[1].split(\" \")[0]\n",
    "        # price\n",
    "        price = laptop.find_element(By.XPATH,\".//span[@class='a-price-whole']\").text.strip()\n",
    "        \n",
    "        if (counter>= 10):\n",
    "            break\n",
    "        else:\n",
    "            data.append([title, rating, price])\n",
    "            counter += 1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Title','Rating','Price']\n",
    "df = pd.DataFrame(data,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01aaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Amazon Laptops.csv\", index=False)\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
